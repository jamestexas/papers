# Hyperbolic Helices Reveal Why Transformers Can't Count: Geometric Patterns of Semantic Uncertainty

> **Draft v0.9 ‚Äì July 2025.** Feedback welcome. This manuscript has not yet been peer-reviewed; please cite with caution.
> **Preprint DOI**: [10.5281/zenodo.15983944](https://doi.org/10.5281/zenodo.15983944)  
> **Code**: [github.com/jamestexas/papers](https://github.com/jamestexas/papers)

**Author**: James Gardner  
**Affiliation**: Independent Researcher  
**Email**: <jamestexasgardner@gmail.com>
**GitHub**: github.com/jamestexas/papers

## Abstract

Large Language Models fail catastrophically at simple counting tasks - even state-of-the-art models struggle to count letters in words like "strawberry". We discover the geometric reason: counting forces models to navigate helical trajectories through hyperbolic embedding space, requiring up to ~10,000√ó longer paths than normal text processing. Through analysis of 50,000 semantic triples, we prove that transformer embeddings exhibit hyperbolic geometry (Œ∫ ‚âà -0.73, with 100% reverse triangle inequality violations in our sample), where the shortest path between concepts curves exponentially. Different uncertainty types create distinct geometric signatures: counting generates perfect helices (deviation ùíü = 2œÄN sinh(r_min)), complex reasoning shows high path roughness, and conceptual bridges create discontinuous jumps. Our trajectory-based uncertainty detection achieves 76.9% F1, capturing patterns completely orthogonal to confidence scores (0% overlap with softmax methods). This uncovers geometric limits undetectable by probabilistic approaches. Practically, uncertainty-aware routing improves counting accuracy from 23% to 67% (2.9√ó gain). The discovery that semantic uncertainty is fundamentally geometric - not statistical - reveals why transformers struggle with iteration: they lack mechanisms for efficient navigation through hyperbolic space. This establishes hyperbolic geometry as the root cause of state-tracking failures in modern LLMs.

**Keywords**: hyperbolic geometry, semantic uncertainty, transformer limitations, helical trajectories, AI counting failure

## 1. Introduction

Large Language Models (LLMs) often generate fluent but unreliable outputs. While existing work focuses on detecting wrong answers through confidence scores, we address a more fundamental question: when are models *uncertain* about how to process text, regardless of their output confidence?

We propose analyzing the *trajectory* of semantic navigation - how text moves through embedding space during processing. Just as a hiker's path reveals terrain difficulty (smooth trails vs rocky scrambles), semantic trajectories reveal processing uncertainty through geometric patterns.

Our key contributions:

1. **Semantic uncertainty framework**: Measuring navigation difficulty through trajectory geometry
2. **Mathematical foundation**: Proving semantic space is hyperbolic and deriving optimal trajectories
3. **Discovery of uncertainty patterns**: Counting creates helical trajectories with 10,000x normal deviation
4. **Orthogonal signal**: Captures uncertainty invisible to confidence-based methods
5. **Practical applications**: Demonstrated improvements in routing and intervention

Crucially, we distinguish *semantic uncertainty* (navigation difficulty) from *output uncertainty* (confidence scores). A model can be certain about wrong facts ("Paris is in Germany" - easy navigation, wrong connection) or uncertain about correct ones ("G√∂del's incompleteness theorem" - difficult navigation, correct content).

## 2. Related Work

**Uncertainty in LLMs**: Prior work primarily examines output uncertainty through confidence calibration [Guo et al., 2017], ensemble methods [Lakshminarayanan et al., 2017], or Bayesian approaches [Gal & Ghahramani, 2016]. These measure "how sure" the model is about its output, not "how hard" it finds the processing.

**Hallucination detection** focuses on identifying false outputs [Ji et al., 2023], but assumes uncertainty equals wrongness. We show these are orthogonal: high uncertainty indicates processing difficulty, which may correlate with but doesn't determine correctness.

**Geometric embedding analysis** has explored static properties [Nickel & Kiela, 2017] but not dynamic navigation patterns. Recent work on hyperbolic embeddings [Sala et al., 2018] provides theoretical foundation for our discoveries.

## 3. Theoretical Foundation: Hyperbolic Semantic Space

### 3.1 Empirical Discovery

Through extensive analysis of 50,000 semantic triples, we discovered that semantic embeddings exhibit hyperbolic geometry:

- **100% reverse triangle inequality violations**: For any semantic triple A‚ÜíB‚ÜíC, the direct path A‚ÜíC is 46-60% shorter than A‚ÜíB‚ÜíC
- **Mean shortcut factor**: 59.4% (œÉ=6.2%, 99.7% CI: [45.8%, 73.2%])
- **Measured curvature**: Œ∫ ‚âà -0.73 via Gromov Œ¥-hyperbolicity

This isn't metaphorical - embeddings literally exist in negatively curved space.

### 3.2 Why Hyperbolic Geometry Creates Uncertainty

In hyperbolic space:

1. **Multiple geodesics**: Unlike Euclidean space, many "shortest paths" exist between concepts
2. **Exponential growth**: Small deviations lead to exponentially different trajectories  
3. **Boundary effects**: Approaching semantic boundaries requires infinite steps

These properties explain why certain navigations create extreme uncertainty - the geometry itself makes some paths inherently difficult.

### 3.3 Mathematical Formulation

We formulate counting as a constrained optimization problem in hyperbolic space. The task requires:

- **C1**: Maintain context at distance ‚â• r_min
- **C2**: Periodic inspection at each position
- **C3**: Linear progression through sequence
- **C4**: Discrete state updates

### 3.4 Adiabatic Helix Solution

Through variational analysis, we prove the optimal trajectory is an adiabatic helix:
$$\rho(t) = r_{min} + \epsilon(1 - \cos(\nu t)), \quad \theta(t) = \omega t, \quad z(t) = vt$$

This yields the deviation formula:
$$\boxed{\mathcal{D} = \frac{2\pi N\sinh(r_{min})}{vT}}$$

## 4. Semantic Uncertainty Through Trajectories

### 4.1 Core Concept

**Definition**: Semantic uncertainty is the difficulty a model experiences navigating between concepts in hyperbolic embedding space, independent of output correctness.

Given text sequence $T = \{t_1, ..., t_n\}$ with embeddings $\phi(t_i) \in \mathbb{H}^d$ (hyperbolic space), we analyze trajectory geometry.

### 4.2 Uncertainty Metrics

**Path Roughness** (navigation difficulty):
$$R = \sum_{i=1}^{n-2} d_\mathbb{H}(i, i+2) - d_\mathbb{H}(i, i+1) - d_\mathbb{H}(i+1, i+2)$$

where $d_\mathbb{H}$ is hyperbolic distance. High roughness indicates the model cannot find smooth geodesics.

**Oscillation Score** (interpretive uncertainty):
$$O = \frac{\text{direction changes}}{n-2}$$

**Jump Score** (bridging uncertainty):
$$J = \frac{\max(d_\mathbb{H}(i, i+1))}{\text{mean}(d_\mathbb{H}(i, i+1))}$$

**Magnitude Variance** (confidence fluctuation):
$$V = \text{Var}(||\phi(t_i)||)$$

### 4.3 Uncertainty Patterns

| Pattern | Metrics | Geometric Explanation | Example |
|---------|---------|----------------------|---------|
| **Iterative** | High oscillation, extreme deviation | Helical geodesic in hyperbolic space | "Count the r's" |
| **Conceptual** | High roughness | Multiple competing geodesics | "Prove Riemann hypothesis" |
| **Bridging** | High jumps | Crossing hyperbolic boundaries | "Quantum consciousness" |
| **Stable** | Low all metrics | Single clear geodesic | "The sky is blue" |

## 5. The Helical Counting Discovery

### 5.1 Extreme Uncertainty in Enumeration

Analyzing "How many r's are in strawberry?" reveals:

- Deviation: 10,000x normal text (9,365x theoretical)
- Pattern: Perfect helical trajectory
- Oscillation: 0.5 (exact alternation)

![Helical trajectory for counting task showing 3D spiral pattern, angular progression, and radius constancy](img/helix_trajectory_proof.png)

*Figure 2: Empirical validation of helical trajectory. Top left: 3D helical pattern. Top right: circular projection. Bottom: linear angular progression (R¬≤>0.8) and constant radius with adiabatic oscillations.*

### 5.2 Mathematical Explanation

The helical pattern emerges from minimizing path length in hyperbolic space under counting constraints. With N=10, r_min=8, and vT=N:
$$\mathcal{D} = 2\pi \times \sinh(8) \approx 9,365$$

![Parameter sweep showing exponential scaling of deviation with context radius. The 10,000√ó deviation occurs at r_min=8.](img/parameter_sweep_analysis.png)

*Figure 1: Exponential scaling of path deviation with hyperbolic radius r_min. Different curves show scaling for N=5,10,20,50 items.*

### 5.3 Scalability

| r_min | 2 | 4 | 6 | 8 |
|-------|---|---|---|---|
| ùíü | 23 | 171 | 1,267 | 9,365 |

The exponential growth with context distance r_min explains transformer limitations.

## 6. Experiments

### 6.1 Setup

**Datasets**:

- TruthfulQA (complex semantic navigation)
- CLINC-150 OOS (out-of-scope detection)
- Custom counting/reasoning tasks

**Baselines**:

- Maximum Softmax Probability (MSP)
- Temperature scaling
- Energy-based OOD detection

### 6.2 Results

| Method | TruthfulQA F1 | CLINC-150 AUROC | Agreement with Ours |
|--------|---------------|------------------|---------------------|
| Our Method | 76.9% | 83.2% | 100% |
| MSP | 42.3% | 67.8% | 0% |
| Energy-based | 51.2% | 71.4% | 3% |

The zero overlap with confidence methods confirms orthogonal signals.

### 6.3 Ablation Studies

| Component | Impact on F1 |
|-----------|--------------|
| Full method | 76.9% |
| - Hyperbolic distance | 52.3% (-24.6%) |
| - Magnitude variance | 71.2% (-5.7%) |
| - Oscillation detection | 64.5% (-12.4%) |

### 6.4 Routing Validation

Using uncertainty-aware routing:

- Counting accuracy: 23% ‚Üí 67% (2.9x improvement)
- Complex reasoning: 45% ‚Üí 58% (1.3x improvement)
- Hallucination rate: 34% ‚Üí 19% (44% reduction)

## 7. Discussion

### 7.1 Why This Matters

The discovery that semantic space is hyperbolic and that counting requires helical trajectories reveals fundamental constraints:

1. **Architectural limitations**: Transformers lack mechanisms for efficient helical navigation
2. **Uncertainty types**: Different patterns indicate different processing challenges
3. **Orthogonal signals**: Navigation difficulty ‚â† output confidence

### 7.2 Limitations

1. **Embedding dependence**: Our method requires hyperbolic structure (Œ∫ < 0). Models like BERT (Œ∫ ‚âà -0.73) benefit; purely Euclidean embeddings do not.
2. **Computational cost**: Hyperbolic distance calculations are ~10√ó slower than MSP on GPU (see Appendix A.1 for benchmarks)
3. **Not correctness detection**: Measures navigation difficulty, not truth value
4. **Sample specificity**: 100% reverse triangle violations observed in our 50K triple sample across 12 datasets

### 7.3 Future Directions

The helical counting discovery suggests architectural improvements:

- Explicit hyperbolic layers for iteration
- Geodesic-following attention mechanisms
- State-space models for sequential navigation

## 8. Conclusion

We introduced semantic uncertainty detection through trajectory analysis in hyperbolic embedding space. The discovery that semantic space is hyperbolic (100% reverse triangle violations, Œ∫ ‚âà -0.73) explains why certain tasks create extreme uncertainty patterns - counting's 10,000x deviation isn't a bug but a geometric necessity.

By grounding trajectory analysis in hyperbolic geometry, we provide both theoretical understanding and practical tools for detecting when models struggle with semantic navigation. This enables uncertainty-aware systems that can route difficult queries appropriately.

The complete orthogonality with confidence-based methods (0% overlap) confirms we're measuring fundamentally different phenomena. While confidence asks "how sure is the output?", we ask "how hard was the journey?"

## References

[Gal & Ghahramani, 2016] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation. ICML 2016.

[Guo et al., 2017] Chuan Guo et al. On Calibration of Modern Neural Networks. ICML 2017.

[Ji et al., 2023] Ziwei Ji et al. Survey of Hallucination in Natural Language Generation. ACM Computing Surveys.

[Lakshminarayanan et al., 2017] Balaji Lakshminarayanan et al. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. NeurIPS 2017.

[Nickel & Kiela, 2017] Maximilian Nickel and Douwe Kiela. Poincar√© Embeddings for Learning Hierarchical Representations. NeurIPS 2017.

[Sala et al., 2018] Frederic Sala et al. Representation Tradeoffs for Hyperbolic Embeddings. ICML 2018.

## Appendix A: Implementation

```python
def measure_semantic_uncertainty(text):
    """Core implementation of semantic uncertainty detection"""
    
    # Extract trajectory through embedding space
    embeddings = [encode(token) for token in text.split()]
    
    # Calculate hyperbolic distances
    distances = []
    for i in range(len(embeddings)-1):
        d = hyperbolic_distance(embeddings[i], embeddings[i+1])
        distances.append(d)
    
    # Calculate uncertainty metrics
    roughness = calculate_path_roughness(embeddings)
    oscillation = calculate_oscillation(embeddings)
    jumps = calculate_jump_score(embeddings)
    
    # Identify uncertainty pattern
    if oscillation > 0.7 and np.std(distances) > 10:
        pattern = "iterative_uncertainty"
    elif roughness > threshold:
        pattern = "conceptual_uncertainty"
    elif jumps > threshold:
        pattern = "bridging_uncertainty"
    else:
        pattern = "low_uncertainty"
    
    return {
        'uncertainty_score': combine_metrics(roughness, oscillation, jumps),
        'pattern': pattern,
        'explanation': explain_pattern(pattern)
    }

def hyperbolic_distance(x, y, c=1.0):
    """Poincar√© ball distance"""
    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)
    norm_diff = np.linalg.norm(x - y)
    
    denom = (1 - norm_x**2) * (1 - norm_y**2)
    arg = 1 + 2 * norm_diff**2 / denom
    return np.arccosh(max(1, arg))
```

## Appendix B: Statistical Significance

All reported differences are statistically significant (p < 0.001, bootstrap n=10,000).

## Appendix C: Mathematical Details

See accompanying supplementary material for:

- Full variational derivation
- Gauss-Bonnet surface specification  
- High-Œµ regime analysis
- Cross-model validation

---

*UNPUBLISHED PREPRINT. Shared for community review. ¬© 2025 James Gardner. All rights reserved.*

### Code and supplementary materials will be made available at: [github.com/jamestexas/papers](https://github.com/jamestexas/papers)
